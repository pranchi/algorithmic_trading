GrammarRandomExpression(grammarDef, 6)
SymRegFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (mean(log(1 + abs(y - result))))
}
set.seed(314)
ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc, terminationCost = 0.1, iterations = 2500, max.depth = 5)
ge
plot(y)
points(eval(ge$best$expressions), col = "red", type = "l")
x <- seq(0, 4*pi, length.out = 201)
y <- jitter(sin(x) + cos(x + x), amount = 0.2)
plot(y)
SymRegFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (mean(log(1 + abs(y - result))))
}
set.seed(314)
ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc, terminationCost = 0.1, iterations = 2500, max.depth = 5)
ge
?GrammaticalEvolution
knitr::opts_chunk$set(echo = TRUE)
library(gramEvol)
x <- seq(0, 4*pi, length.out = 201)
y <- sin(x) + cos(x + x)
plot(y)
library("gramEvol")
ruleDef <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos),
op = grule('+', '-', '*'),
var = grule(x))
grammarDef <- CreateGrammar(ruleDef)
grammarDef
set.seed(123)
GrammarRandomExpression(grammarDef, 6)
SymRegFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (mean(log(1 + abs(y - result))))
}
set.seed(314)
ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc, terminationCost = 0.1, iterations = 2500, max.depth = 5)
ge
plot(y)
points(eval(ge$best$expressions), col = "red", type = "l")
x <- seq(0, 4*pi, length.out = 201)
y <- jitter(sin(x) + cos(x + x), amount = 0.2)
plot(y)
SymRegFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (mean(log(1 + abs(y - result))))
}
set.seed(314)
ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc, terminationCost = 0.1, iterations = 2500, max.depth = 5)
ge
plot(y)
points(eval(ge$best$expressions), col = "red", type = "l")
planets <- c("Venus", "Earth", "Mars", "Jupiter", "Saturn", "Uranus")
distance <- c(0.72, 1.00, 1.52, 5.20, 9.53, 19.10)
period <- c(0.61, 1.00, 1.84, 11.90, 29.40, 83.50)
data.frame(planets, distance, period)
ruleDef <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, tan, log, sqrt),
op = grule('+', '-', '*', '/', '^'),
var = grule(distance, n),
n = grule(1, 2, 3, 4, 5, 6, 7, 8, 9))
grammarDef <- CreateGrammar(ruleDef)
grammarDef
SymRegFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (mean(log(1 + abs(period - result))))
}
set.seed(2)
suppressWarnings(ge <- GrammaticalEvolution(grammarDef, SymRegFitFunc, terminationCost = 0.05))
ge
library(quantmod)
# generate the series
len <- 20  # for example
fibvals <- integer(len)
fibvals[1] <- 1L
fibvals[2] <- 1L
for (i in 3:len) {
fibvals[i] <- fibvals[i-1]+fibvals[i-2]
}
# check the contents of this
fibvals
# #Create a lagged dataframe of the data
# fibdat <- data.frame(fibvals, x1=Lag(fibvals,1), x2=Lag(fibvals,2))
# names(fibdat) <- c('x','x1','x2')
#
# # drop first two rows as these will contain NaNs
# fibdat <- fibdat[c(3:20),]
#
# #have a look at this
# fibdat
library(quantmod)
# generate the series
len <- 20  # for example
fibvals <- integer(len)
fibvals[1] <- 1L
fibvals[2] <- 1L
# for (i in 3:len) {
#   fibvals[i] <- fibvals[i-1]+fibvals[i-2]
# }
# check the contents of this
# fibvals
# #Create a lagged dataframe of the data
# fibdat <- data.frame(fibvals, x1=Lag(fibvals,1), x2=Lag(fibvals,2))
# names(fibdat) <- c('x','x1','x2')
#
# # drop first two rows as these will contain NaNs
# fibdat <- fibdat[c(3:20),]
#
# #have a look at this
# fibdat
library(quantmod)
# generate the series
len <- 20  # for example
fibvals <- integer(len)
# fibvals[1] <- 1L
# fibvals[2] <- 1L
# for (i in 3:len) {
#   fibvals[i] <- fibvals[i-1]+fibvals[i-2]
# }
# check the contents of this
# fibvals
# #Create a lagged dataframe of the data
# fibdat <- data.frame(fibvals, x1=Lag(fibvals,1), x2=Lag(fibvals,2))
# names(fibdat) <- c('x','x1','x2')
#
# # drop first two rows as these will contain NaNs
# fibdat <- fibdat[c(3:20),]
#
# #have a look at this
# fibdat
fibvals[1] <- 1L
fibvals[2] <- 1L
for (i in 3:len) {
fibvals[i] <- fibvals[i-1]+fibvals[i-2]
}
# #Create a lagged dataframe of the data
fibdat <- data.frame(fibvals, x1=Lag(fibvals,1), x2=Lag(fibvals,2))
View(fibdat)
View(fibdat)
names(fibdat) <- c('x','x1','x2')
# # drop first two rows as these will contain NaNs
fibdat <- fibdat[c(3:20),]
#
# #have a look at this
fibdat
somedata <- data.frame(x3=Lag(somevals,3), x2=Lag(somevals,2), x1=Lag(somevals,1), somevals)
library(gramEvol)
library(quantmod)
somevals<-c(0, 1, 3, 2, 3, 6, 8, 10, 14, 12, 18, 26, 23, 29, 31, 34, 32, 35, 39, 40, 42, 37, 43, 47, 44)
somedata <- data.frame(x3=Lag(somevals,3), x2=Lag(somevals,2), x1=Lag(somevals,1), somevals)
names(somedata) <- c('x3','x2','x1','x')
View(somedata)
traindata <- somedata[4:20,]
testdata <- somedata[21:25,]
newFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
newGram <- CreateGrammar(newRules)
newRules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
newFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
newGram <- CreateGrammar(newRules)
mydata <- traindata
ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.05, max.depth = 5)
ge
ge$best$expressions
eval(ge$best$expressions)
View(traindata)
traindata <- somedata[4:20,]
testdata <- somedata[21:25,]
newRules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
newFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
newGram <- CreateGrammar(newRules)
mydata <- traindata
View(mydata)
suppressWarnings(ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.01, max.depth = 5))
suppressWarnings(ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.01, max.depth = 5))
ge
ge$best$expressions
ge$best$expressions
eval(ge$best$expressions)
mydata <- testdata
eval(ge$best$expressions)
mydata <-testdata[1,1:3]
predictions <-c()
for(i in 1:5){
predictions[i] <- eval(ge$best$expressions)
# shuffle data along and insert new prediction
# put this in a loop for a large window size
mydata[3] <- mydata[2]
mydata[2] <- mydata[1]
mydata[1] <- predictions[i]}
mydata <-testdata[1,1:3]
View(mydata)
mydata <-testdata[1,1:3]
predictions <-c()
for(i in 1:5){
predictions[i] <- eval(ge$best$expressions)
# shuffle data along and insert new prediction
# put this in a loop for a large window size
mydata[3] <- mydata[2]
mydata[2] <- mydata[1]
mydata[1] <- predictions[i]}
actuals <- testdata$x
sqrt(mean(actuals - predictions)^2)
actuals
predictions
someStocks <- c("NFLX")
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
View(NFLX)
someStocks <- c("NFLX")
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata.ts <- as.xts(NFLX$NFLX.Adjusted)
plot.ts(mynflxdata.ts)
mynflxcomponents <- decompose(mynflxdata.ts)
View(mynflxdata)
View(mynflxdata.ts)
View(mynflxdata.ts)
View(mynflxdata)
View(mynflxdata)
library(quantmod)
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata
mynflxdata
View(mynflxdata)
names(mynflxdata) <- c('x3','x2','x1','x')
View(mynflxdata)
mynflxdata(3:,)
mynflxdata((3:,))
mynflxdata[c(3:),)]
mynflxdata[c(3:),]
mynflxdata[c(3:10),]
length(mynflxdata)
nrow(mynflxdata)
nrow(mynflxdata[c(3:-1),])
nrow(mynflxdata[c(3:),])
nrow(mynflxdata[c(3:,)])
nrow(mynflxdata[c(c(3:),)])
nrow(mynflxdata[3:,])
nrow(mynflxdata[c(3:nrow(mynflxdata)),])
mynflxdata <- (mynflxdata[c(3:nrow(mynflxdata)),])
View(mynflxdata)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata
mynflxdata <- (mynflxdata[c(4:nrow(mynflxdata)),])
View(mynflxdata)
test <-
train <- mynflxdata[c(4:nrow(mynflxdata)-100),]
nrow(train)
test <-
train <- mynflxdata[c(4:nrow(mynflxdata)-100),]
nrow(train)
test <-
train <- mynflxdata[c(4:nrow(mynflxdata)-100),]
test <-
train <- mynflxdata[c(4:(nrow(mynflxdata)-100)),]
View(train)
View(train)
View(train)
View(train)
View(traindata)
View(test)
View(test)
View(train)
library(quantmod)
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
# test <-
train <- mynflxdata[c(4:(nrow(mynflxdata)-100)),]
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
# test <-
train <- mynflxdata[c(4:(nrow(mynflxdata)-100)),]
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
nrow(mynflxdata)
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
nrow(mynflxdata)
# test <-
train <- mynflxdata[c(1:(nrow(mynflxdata)-100)),]
nrow(train)
test <- mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata),)]
test <- mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata)),]
nrow(test)
nrow(mynflxdata)-100
mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata)),]
nrow(mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata)),])
test <- mynflxdata[c((nrow(mynflxdata)-100):nrow(mynflxdata)),]
nrow(test)
library(quantmod)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-101):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-100)),]
nrow(test)
library(quantmod)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-99):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-100)),]
nrow(test)
nrow(train)
library(quantmod)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-49):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-50)),]
nrow(train)
nrow(test)
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
library(gramEvol)
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
ruleGrammar <- CreateGrammar(rules)
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction, terminationCost = 0.05, max.depth = 5)
fitnessFunction <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
mydata <- train
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction, terminationCost = 0.05, max.depth = 5)
ge
source("~/.active-rstudio-document", echo=TRUE)
ge$best$expressions
library(quantmod)
library(gramEvol)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-49):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-50)),]
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
ruleGrammar <- CreateGrammar(rules)
fitnessFunction <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
mydata <- train
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction, terminationCost = 0.05, max.depth = 5)
ge$best$expressions
eval(ge$best$expressions)
View(train)
library(quantmod)
library(gramEvol)
stock <- 'NFLX'
fromDate <- '2017-01-01'
toDate <- '2017-02-01'
windowSize <- 5
testTrainSplit <- .20
# Read Data
getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
dataAdjusted <- NFLX$NFLX.Adjusted
dataOpen <- NFLX$NFLX.Open
dataClose <- NFLX$NFLX.Close
dataLow <- NFLX$NFLX.Low
dataHigh <- NFLX$NFLX.High
dataVolume <- NFLX$NFLX.Volume
dataPrediction <- function(data){
# Converting to DataFrame by lagging the dataset for prediction.
# Also, removing NA values
# data <- dataAdjusted
stocksData <- data.frame(Lag(data,seq(from=windowSize,to=1)),
data)[(windowSize+1):nrow(data),]
colNames <- c()
for (i in 1:windowSize){
colNames[i] <- paste('x', i, sep='')
}
colNames <- colNames[windowSize:1]
colNames[i+1] <- 'x'
names(stocksData) <- colNames
# split test and train data
train <- head(stocksData, floor(nrow(stocksData)*(1-testTrainSplit)))
test <- tail(stocksData, ceiling(nrow(stocksData)*testTrainSplit))
# Generate Rule for expression
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
# TODO populate variables dynamically [cant figure out this one,
# will have to change along with change in window size]
var = grule(mydata$x5, mydata$x4, mydata$x3,
mydata$x2, mydata$x1),
rn = gvrule(runif(100,-10,10))
)
ruleGrammar <- CreateGrammar(rules)
# fitness function for calculating best expression
fitnessFunction <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)) || any(is.na(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
# Training using GE
mydata <- train
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction,
terminationCost = 0.05, max.depth = 5)
predictions <- eval(ge$best$expressions)
# Testing using GE
mydata <- test
predictions <- eval(ge$best$expressions)
mydata <- cbind(mydata, predictions)
# Next n days predictions
mydata <- tail(mydata,1)[,c(1:windowSize)]
variety_predictions <- c()
for(i in 1:5){
variety_predictions[i] <- eval(ge$best$expressions)
for (j in (windowSize:2)){
mydata[j]<-mydata[j-1]
}
mydata[1] <- variety_predictions[i]
}
return (variety_predictions)
}
adjustedPredictions <- dataPrediction(dataAdjusted)
openPredictions <- dataPrediction(dataOpen)
predictions <- data.frame(cbind(adjustedPredictions, openPredictions))
# prediction for 2017-02-01 to 2017-02-06 in variety_predictions
# getSymbols(stock, src="yahoo", from=toDate, to='2017-02-10', freq="daily")
# b <- cbind(head(NFLX$NFLX.Adjusted,5), variety_predictions)
View(predictions)
library(neuralnet)
library(quantmod)
library(nnet, lib.loc = "C:/Program Files/R/R-4.1.2/library")
detach("package:nnet", unload = TRUE)
install.packages("neuralnet")
library(neuralnet)
library(neuralnet)
library(quantmod)
chartSeries(NFLX)
```{r 2 Dataset, echo=FALSE, eval= TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(gramEvol)
library(quantmod)
stock <- 'NFLX'
fromDate <- '2017-01-01'
toDate <- '2017-02-01'
windowSize <- 20
testTrainSplit <- .20
# Read Data
getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
dataAdjusted <- NFLX$NFLX.Adjusted
dataOpen <- NFLX$NFLX.Open
dataClose <- NFLX$NFLX.Close
dataLow <- NFLX$NFLX.Low
dataHigh <- NFLX$NFLX.High
dataVolume <- NFLX$NFLX.Volume
chartSeries(NFLX)

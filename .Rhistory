fibvals[1] <- 1L
fibvals[2] <- 1L
# for (i in 3:len) {
#   fibvals[i] <- fibvals[i-1]+fibvals[i-2]
# }
# check the contents of this
# fibvals
# #Create a lagged dataframe of the data
# fibdat <- data.frame(fibvals, x1=Lag(fibvals,1), x2=Lag(fibvals,2))
# names(fibdat) <- c('x','x1','x2')
#
# # drop first two rows as these will contain NaNs
# fibdat <- fibdat[c(3:20),]
#
# #have a look at this
# fibdat
library(quantmod)
# generate the series
len <- 20  # for example
fibvals <- integer(len)
# fibvals[1] <- 1L
# fibvals[2] <- 1L
# for (i in 3:len) {
#   fibvals[i] <- fibvals[i-1]+fibvals[i-2]
# }
# check the contents of this
# fibvals
# #Create a lagged dataframe of the data
# fibdat <- data.frame(fibvals, x1=Lag(fibvals,1), x2=Lag(fibvals,2))
# names(fibdat) <- c('x','x1','x2')
#
# # drop first two rows as these will contain NaNs
# fibdat <- fibdat[c(3:20),]
#
# #have a look at this
# fibdat
fibvals[1] <- 1L
fibvals[2] <- 1L
for (i in 3:len) {
fibvals[i] <- fibvals[i-1]+fibvals[i-2]
}
# #Create a lagged dataframe of the data
fibdat <- data.frame(fibvals, x1=Lag(fibvals,1), x2=Lag(fibvals,2))
View(fibdat)
View(fibdat)
names(fibdat) <- c('x','x1','x2')
# # drop first two rows as these will contain NaNs
fibdat <- fibdat[c(3:20),]
#
# #have a look at this
fibdat
somedata <- data.frame(x3=Lag(somevals,3), x2=Lag(somevals,2), x1=Lag(somevals,1), somevals)
library(gramEvol)
library(quantmod)
somevals<-c(0, 1, 3, 2, 3, 6, 8, 10, 14, 12, 18, 26, 23, 29, 31, 34, 32, 35, 39, 40, 42, 37, 43, 47, 44)
somedata <- data.frame(x3=Lag(somevals,3), x2=Lag(somevals,2), x1=Lag(somevals,1), somevals)
names(somedata) <- c('x3','x2','x1','x')
View(somedata)
traindata <- somedata[4:20,]
testdata <- somedata[21:25,]
newFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
newGram <- CreateGrammar(newRules)
newRules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
newFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
newGram <- CreateGrammar(newRules)
mydata <- traindata
ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.05, max.depth = 5)
ge
ge$best$expressions
eval(ge$best$expressions)
View(traindata)
traindata <- somedata[4:20,]
testdata <- somedata[21:25,]
newRules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
newFitFunc <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
newGram <- CreateGrammar(newRules)
mydata <- traindata
View(mydata)
suppressWarnings(ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.01, max.depth = 5))
suppressWarnings(ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.01, max.depth = 5))
ge
ge$best$expressions
ge$best$expressions
eval(ge$best$expressions)
mydata <- testdata
eval(ge$best$expressions)
mydata <-testdata[1,1:3]
predictions <-c()
for(i in 1:5){
predictions[i] <- eval(ge$best$expressions)
# shuffle data along and insert new prediction
# put this in a loop for a large window size
mydata[3] <- mydata[2]
mydata[2] <- mydata[1]
mydata[1] <- predictions[i]}
mydata <-testdata[1,1:3]
View(mydata)
mydata <-testdata[1,1:3]
predictions <-c()
for(i in 1:5){
predictions[i] <- eval(ge$best$expressions)
# shuffle data along and insert new prediction
# put this in a loop for a large window size
mydata[3] <- mydata[2]
mydata[2] <- mydata[1]
mydata[1] <- predictions[i]}
actuals <- testdata$x
sqrt(mean(actuals - predictions)^2)
actuals
predictions
someStocks <- c("NFLX")
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
View(NFLX)
someStocks <- c("NFLX")
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata.ts <- as.xts(NFLX$NFLX.Adjusted)
plot.ts(mynflxdata.ts)
mynflxcomponents <- decompose(mynflxdata.ts)
View(mynflxdata)
View(mynflxdata.ts)
View(mynflxdata.ts)
View(mynflxdata)
View(mynflxdata)
library(quantmod)
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata
mynflxdata
View(mynflxdata)
names(mynflxdata) <- c('x3','x2','x1','x')
View(mynflxdata)
mynflxdata(3:,)
mynflxdata((3:,))
mynflxdata[c(3:),)]
mynflxdata[c(3:),]
mynflxdata[c(3:10),]
length(mynflxdata)
nrow(mynflxdata)
nrow(mynflxdata[c(3:-1),])
nrow(mynflxdata[c(3:),])
nrow(mynflxdata[c(3:,)])
nrow(mynflxdata[c(c(3:),)])
nrow(mynflxdata[3:,])
nrow(mynflxdata[c(3:nrow(mynflxdata)),])
mynflxdata <- (mynflxdata[c(3:nrow(mynflxdata)),])
View(mynflxdata)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata
mynflxdata <- (mynflxdata[c(4:nrow(mynflxdata)),])
View(mynflxdata)
test <-
train <- mynflxdata[c(4:nrow(mynflxdata)-100),]
nrow(train)
test <-
train <- mynflxdata[c(4:nrow(mynflxdata)-100),]
nrow(train)
test <-
train <- mynflxdata[c(4:nrow(mynflxdata)-100),]
test <-
train <- mynflxdata[c(4:(nrow(mynflxdata)-100)),]
View(train)
View(train)
View(train)
View(train)
View(traindata)
View(test)
View(test)
View(train)
library(quantmod)
getSymbols(someStocks, src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
mynflxdata
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
# test <-
train <- mynflxdata[c(4:(nrow(mynflxdata)-100)),]
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
# test <-
train <- mynflxdata[c(4:(nrow(mynflxdata)-100)),]
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
nrow(mynflxdata)
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
nrow(mynflxdata)
# test <-
train <- mynflxdata[c(1:(nrow(mynflxdata)-100)),]
nrow(train)
test <- mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata),)]
test <- mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata)),]
nrow(test)
nrow(mynflxdata)-100
mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata)),]
nrow(mynflxdata[c(nrow(mynflxdata)-100:nrow(mynflxdata)),])
test <- mynflxdata[c((nrow(mynflxdata)-100):nrow(mynflxdata)),]
nrow(test)
library(quantmod)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-101):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-100)),]
nrow(test)
library(quantmod)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-99):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-100)),]
nrow(test)
nrow(train)
library(quantmod)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-49):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-50)),]
nrow(train)
nrow(test)
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
library(gramEvol)
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
ruleGrammar <- CreateGrammar(rules)
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction, terminationCost = 0.05, max.depth = 5)
fitnessFunction <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
mydata <- train
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction, terminationCost = 0.05, max.depth = 5)
ge
source("~/.active-rstudio-document", echo=TRUE)
ge$best$expressions
library(quantmod)
library(gramEvol)
getSymbols('NFLX', src="yahoo", from="2020-01-01", to="2021-01-01", freq="daily")
plot(NFLX$NFLX.Adjusted)
mynflxdata <- data.frame(x3=Lag(NFLX$NFLX.Adjusted,3), x2=Lag(NFLX$NFLX.Adjusted,2), x1=Lag(NFLX$NFLX.Adjusted,1), NFLX$NFLX.Adjusted)
names(mynflxdata) <- c('x3','x2','x1','x')
mynflxdata <- mynflxdata[c(4:nrow(mynflxdata)),]
test <- mynflxdata[c((nrow(mynflxdata)-49):nrow(mynflxdata)),]
train <- mynflxdata[c(1:(nrow(mynflxdata)-50)),]
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
var = grule(mydata$x3, mydata$x2, mydata$x1))
ruleGrammar <- CreateGrammar(rules)
fitnessFunction <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
mydata <- train
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction, terminationCost = 0.05, max.depth = 5)
ge$best$expressions
eval(ge$best$expressions)
View(train)
library(quantmod)
library(gramEvol)
stock <- 'NFLX'
fromDate <- '2017-01-01'
toDate <- '2017-02-01'
windowSize <- 5
testTrainSplit <- .20
# Read Data
getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
dataAdjusted <- NFLX$NFLX.Adjusted
dataOpen <- NFLX$NFLX.Open
dataClose <- NFLX$NFLX.Close
dataLow <- NFLX$NFLX.Low
dataHigh <- NFLX$NFLX.High
dataVolume <- NFLX$NFLX.Volume
dataPrediction <- function(data){
# Converting to DataFrame by lagging the dataset for prediction.
# Also, removing NA values
# data <- dataAdjusted
stocksData <- data.frame(Lag(data,seq(from=windowSize,to=1)),
data)[(windowSize+1):nrow(data),]
colNames <- c()
for (i in 1:windowSize){
colNames[i] <- paste('x', i, sep='')
}
colNames <- colNames[windowSize:1]
colNames[i+1] <- 'x'
names(stocksData) <- colNames
# split test and train data
train <- head(stocksData, floor(nrow(stocksData)*(1-testTrainSplit)))
test <- tail(stocksData, ceiling(nrow(stocksData)*testTrainSplit))
# Generate Rule for expression
rules <- list(expr = grule(op(expr, expr), func(expr), var),
func = grule(sin, cos, exp, log),
op = grule('+', '-', '*', '/', '^'),
# TODO populate variables dynamically [cant figure out this one,
# will have to change along with change in window size]
var = grule(mydata$x5, mydata$x4, mydata$x3,
mydata$x2, mydata$x1),
rn = gvrule(runif(100,-10,10))
)
ruleGrammar <- CreateGrammar(rules)
# fitness function for calculating best expression
fitnessFunction <- function(expr) {
result <- eval(expr)
if (any(is.nan(result)) || any(is.na(result)))
return(Inf)
return (sqrt(mean((mydata$x - result)^2)))
}
# Training using GE
mydata <- train
ge <- GrammaticalEvolution(ruleGrammar, fitnessFunction,
terminationCost = 0.05, max.depth = 5)
predictions <- eval(ge$best$expressions)
# Testing using GE
mydata <- test
predictions <- eval(ge$best$expressions)
mydata <- cbind(mydata, predictions)
# Next n days predictions
mydata <- tail(mydata,1)[,c(1:windowSize)]
variety_predictions <- c()
for(i in 1:5){
variety_predictions[i] <- eval(ge$best$expressions)
for (j in (windowSize:2)){
mydata[j]<-mydata[j-1]
}
mydata[1] <- variety_predictions[i]
}
return (variety_predictions)
}
adjustedPredictions <- dataPrediction(dataAdjusted)
openPredictions <- dataPrediction(dataOpen)
predictions <- data.frame(cbind(adjustedPredictions, openPredictions))
# prediction for 2017-02-01 to 2017-02-06 in variety_predictions
# getSymbols(stock, src="yahoo", from=toDate, to='2017-02-10', freq="daily")
# b <- cbind(head(NFLX$NFLX.Adjusted,5), variety_predictions)
View(predictions)
library(neuralnet)
library(quantmod)
library(nnet, lib.loc = "C:/Program Files/R/R-4.1.2/library")
detach("package:nnet", unload = TRUE)
install.packages("neuralnet")
library(neuralnet)
library(neuralnet)
library(quantmod)
chartSeries(NFLX)
```{r 2 Dataset, echo=FALSE, eval= TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(gramEvol)
library(quantmod)
stock <- 'NFLX'
fromDate <- '2017-01-01'
toDate <- '2017-02-01'
windowSize <- 20
testTrainSplit <- .20
# Read Data
getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
dataAdjusted <- NFLX$NFLX.Adjusted
dataOpen <- NFLX$NFLX.Open
dataClose <- NFLX$NFLX.Close
dataLow <- NFLX$NFLX.Low
dataHigh <- NFLX$NFLX.High
dataVolume <- NFLX$NFLX.Volume
chartSeries(NFLX)
ls
dir
knitr::opts_chunk$set(echo = TRUE)
library (quantmod)
library (knitr)
library (RColorBrewer)
library (GA)
library (ggplot2)
library(doParallel)
library(parallel)
options("getSymbols.warning4.0"=FALSE)
coul <- brewer.pal(5, 'Set2')
consumerElectronics <- c('DELL', 'IT', 'NVDA', 'HPE', 'FTNT',
'AAPL', 'AMZN', 'MSFT', 'GOOG', 'SONY')
financialStocks <- c('SC', 'BEN', 'SBNY', 'EVR','AFG',
'WFC', 'GS', 'MS', 'JPM', 'BAC')
healthCareStocks <- c('BIO', 'HCA', 'PFE', 'QGEN', 'ANTM',
'UNH','ABBV', 'MRNA', 'NVO', 'SAGE')
communicationStocks <- c('VIAC', 'ROKU', 'LYV', 'EA', 'NXST',
'T', 'VZ', 'NTTYY','DTEGY','TMUS')
consumerGoodsStocks <- c('EBAY', 'TSLA', 'GME', 'MAR', 'FL',
'HD', 'MCD', 'NKE', 'WMT', 'PEP')
myStocks <- c(consumerElectronics[1:2],financialStocks[1:2],
healthCareStocks[1:2], communicationStocks[1:2],
consumerGoodsStocks[1:2])
Stocks <- lapply(myStocks,
function(sym) {
dailyReturn(na.omit(getSymbols(sym,
from='2020-01-01',
to='2021-01-01',
src = 'yahoo',
auto.assign=FALSE)))})
myRetData <- do.call(merge, Stocks)
colnames(myRetData) <- myStocks
meanDailyReturns <- apply(myRetData, 2, mean)
sigma <- cov(myRetData)
barplot(meanDailyReturns, col=coul,
xlab = 'Portfolio Assets',
cex.names = .5,
ylab = 'Mean',
main = 'Mean Daily Returns of Assets')
# Function to generate random set of weights
randomWeight <- function(){
weightRandom <- runif(length(myStocks),min = 0,max = 1)
weightRandom <- weightRandom/sum(weightRandom)
return (weightRandom)
}
# Function to generate weighted risk of assets
weightedRisk <-function(sgma, wt){
rsk <- 0
for (i in seq(1:length(wt))){
for (j in seq(1:length(wt))){
rsk <- rsk + (sgma[i,j] * wt[i] * wt[j])
}
}
return(rsk)
}
# Function to create weighted returns of the assets
weightedReturn <- function(rtrn, wt){
return (sum(wt*rtrn))
}
# Function to Calculate the fitness function by using weighted returns and risk
riskVector <- c()
returnVector <- c()
fitWeight <- function(wt){
wt <- wt/sum(wt)
risk <<- weightedRisk(sigma, wt)
returns <<- weightedReturn(meanDailyReturns, wt)
riskVector <<- c(riskVector, risk)
returnVector <<- c(returnVector, returns)
return (returns / risk)
}
GAOptimumWeights <- ga(type = "real-valued", fitness = fitWeight,
maxiter = 100, popSize = 100, lower = c(rep(0,length(myStocks))),
upper = c(rep(1,length(myStocks))),
monitor = FALSE)
optimumWeight <- GAOptimumWeights@solution/sum(GAOptimumWeights@solution)
portfolioPerformance <- cbind(returns, risk, GAOptimumWeights@fitnessValue)
colnames(portfolioPerformance) <- c('Returns',
'Risk', 'Fitness')
kable(portfolioPerformance)
kable(round(optimumWeight,3))
plot(GAOptimumWeights, main='GA Performance')
# Read Data
stockData <- getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
stockData
stockData$NFLX
library(gramEvol)
library(quantmod)
# Read Data
stockData <- getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
stock <- 'NFLX'
fromDate <- '2017-01-01'
toDate <- '2017-02-01'
# Read Data
stockData <- getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
# Read Data
stockData <- lapply(stock, function(sym) {
dailyReturn(na.omit(getSymbols(sym, from=fromDate, to=toDate, src = 'yahoo', auto.assign=FALSE)))
})
stockData
# Read Data
stockData <- getSymbols(stock, src="yahoo", from=fromDate, to=toDate, freq="daily")
get(stockData)
get(stockData).NFLX
get(stockData).Adjusted
x <- get(stockData)
get(stockData)$NFLX.Adjusted
